{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Twitter (Based on Chapter 2 of the book 'Mastering...')\n",
    "\n",
    "In this notebook you will learn the basic concepts of Twitter mining and will try to gather some data from Twitter. Follow the steps, and do the exercises in a copy of this notebook. For that, click in *File -> Make a copy...* button. Don't forget to rename it.\n",
    "\n",
    "To include a new cell in the notebook, click on the plus sign on the toolbar, or tipe *Alt+Enter* to execute the current cell and create a new one. You can also click on the *Cell* menu.\n",
    "\n",
    "## Important\n",
    "You have to run all the commands of this document, and make sure that they were executed well. After you finish, you have to save it as PDF and submit. Click in File -> Download as -> PDF via Latex to do so. \n",
    "\n",
    "**If you don't have Latex at your computer**, simply use File -> Print Preview and generate the PDF from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing libraries\n",
    "If one of the libraries is missing, go to Anaconda, at the Environments tab on the left side of the window and find and install your library.\n",
    "\n",
    "![Anaconda](./Pic16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweepy, Json and Python\n",
    "\n",
    "To collect data and make it readable, two libraries will be used. Tweepy is a library that embed the twitter API functions making it easier to interact with the Twitter server.\n",
    "\n",
    "Json is the format that the data is extracted from Twitter. For this reason, the library Json is also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own key at Twitter webpage\n",
    "You need your own keys to have access to Twitter API. For that, go to https://apps.twitter.com/ then create a new App, and generate the keys. \n",
    "\n",
    "In case you have problems, watch the 5 first minutes of https://www.youtube.com/watch?v=pUUxmvvl2FE only. It is more than enough for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put your keys here!\n",
    "\n",
    "The variables where you should put your keys are below. So copy and paste all the 4 keys and paste them below.\n",
    "\n",
    "The first part of this notebook shows you the Tweepy library being loaded, together with the os and sys libraries. Those are used to check files at your system and yield system messages in case it is necessary.\n",
    "\n",
    "The two functions basically will read your tokens and keys and generate an object that can communicate with Twitter and ask for data. Don't be scared at first moment! Those functions are default for all twitter applications, and are very ugly, indeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_auth():\n",
    "    \"\"\"Setup Twitter authentication.\n",
    "\n",
    "    Return: tweepy.OAuthHandler object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        consumer_key = ''# os.environ['TWITTER_CONSUMER_KEY']\n",
    "        consumer_secret = '' # os.environ['TWITTER_CONSUMER_SECRET']\n",
    "        access_token = '' # os.environ['TWITTER_ACCESS_TOKEN']\n",
    "        access_secret = '' # os.environ['TWITTER_ACCESS_SECRET']\n",
    "    except KeyError:\n",
    "        sys.stderr.write(\"TWITTER_* environment variables not set\\n\")\n",
    "        sys.exit(1)    \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_client():\n",
    "    \"\"\"Setup Twitter API client.\n",
    "\n",
    "    Return: tweepy.API object\n",
    "    \"\"\"\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tweets from your timeline\n",
    "So the first thing I'm gonna do is to read my own tweets from my own timeline. For that, I'll use more stuff from Tweepy (the cursor) and also the library Json. The data dumped on us after we request something to Python API is in Json format. If you wanna learn more about that, feel free to enjoy http://www.w3schools.com/js/js_json_intro.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below there is no need for changes in the code. the client.home_timeline is the query for the Cursor, that will return 200 x 4 = 800 tweets from your home (that is the maximum you can get from your own timeline).\n",
    "\n",
    "**Notice** that a file named home_timeline.jsonl will be generated on the same folder as you saved this notebook! You can open this file to see a very messy information about all your tweets (one per line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = get_twitter_client()\n",
    "\n",
    "with open('home_timeline.jsonl', 'w') as f:\n",
    "    for page in Cursor(client.home_timeline, count=200, include_rts=True).pages(4):\n",
    "        for status in page:\n",
    "            # Process a single status\n",
    "            f.write(json.dumps(status._json, indent=4)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1\n",
    "\n",
    "Open the file called home_timeline.jsonl and read it. Use a text editor for that, like Gedit (Linux), Notepad (Windows) or TextEdit (Mac OSx).\n",
    "1. What kind of information do you see in the file?\n",
    "2. Can you get your tweets from the file? If yes, how?\n",
    "3. Can you say what is the code doing by just reading the lines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tweets from another user's timeline\n",
    "You can also get information about another user's timeline. And the good news are: you can get more than the 800 tweets you got before! But now you have to give the username of the twitter account you wanna take the tweets. We will read 3200 tweets from VUamsterdam account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give the name of the account you want to crawl the data from.\n",
    "user = 'VUamsterdam'\n",
    "fname = \"user_timeline_{}.jsonl\".format(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collecting 16 pages of 200 tweets each.\n",
    "with open(fname, 'w') as f:\n",
    "    for page in Cursor(client.user_timeline, screen_name=user, count=200).pages(16):\n",
    "        for status in page:\n",
    "            f.write(json.dumps(status._json)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good to know!\n",
    "Using the Cursor to find tweets from your own account gives you a limit of 800 tweets maximum. When you use the second function, finding the tweets in a user timeline, this limit increaes to 3200.\n",
    "\n",
    "It is also good to know the structure of a tweet. In our code above the *_json* attribute is the one saved in the file. This attribute contains all the information from the tweet in a JSON format. The JSON format is useful to keep the structure of the information to be saved, and it is the same as a dictionary. In our case, we are creating a jsonl file, or a list of jsons, because every line corresponds to the information of a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Analysis\n",
    "\n",
    "Now that we can get the tweets in a timeline, we can also extract the hashtags and produce the list with the more common ones.\n",
    "\n",
    "Remember that you have to change the name of your input (fname) if you are investigating another account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_hashtags(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('hashtags', [])\n",
    "    return [tag['text'].lower() for tag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the file saved and extract the hashtags\n",
    "with open(fname, 'r') as f:\n",
    "    hashtags = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        hashtags_in_tweet = get_hashtags(tweet)\n",
    "        hashtags.update(hashtags_in_tweet)\n",
    "    for tag, count in hashtags.most_common(20):\n",
    "        print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question about your data set:\n",
    "1. Do you have any insight by looking at the hashtags found in the page you are verifying? Can you explain why those hashtags are the most used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going deeper...\n",
    "\n",
    "We can have deeper analysis of the statistics of the hashtags, instead of a simple counting of the number of times each one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(fname, 'r') as f:\n",
    "    hashtag_count = defaultdict(int)\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        hashtags_in_tweet = get_hashtags(tweet)\n",
    "        n_of_hashtags = len(hashtags_in_tweet)\n",
    "        hashtag_count[n_of_hashtags] += 1\n",
    "\n",
    "    tweets_with_hashtags = sum([count for n_of_tags, count in hashtag_count.items() if n_of_tags > 0])\n",
    "    tweets_no_hashtags = hashtag_count[0]\n",
    "    tweets_total = tweets_no_hashtags + tweets_with_hashtags\n",
    "    # print tweets_total\n",
    "    tweets_with_hashtags_percent = '{:2f}'.format(tweets_with_hashtags * 100.0 / tweets_total )\n",
    "    tweets_no_hashtags_percent = \"{:2f}\".format(tweets_no_hashtags * 100.0 / tweets_total )\n",
    "    # print tweets_no_hashtags_percent\n",
    "    print(\"{0} tweets without hashtags ({1}%)\".format(tweets_no_hashtags, tweets_no_hashtags_percent))\n",
    "    print(\"{} tweets with at least one hashtag ({}%)\".format(tweets_with_hashtags, tweets_with_hashtags_percent))\n",
    "\n",
    "    for tag_count, tweet_count in hashtag_count.items():\n",
    "        if tag_count > 0:\n",
    "            percent_total = \"%.2f\" % float(tweet_count * 100.0 / tweets_total )\n",
    "            percent_elite = \"%.2f\" % float(tweet_count * 100.0 / tweets_with_hashtags)\n",
    "            print(\"{} tweets with {} hashtags ({}% total, {}% elite)\".format(tweet_count,\n",
    "                                                                             tag_count,\n",
    "                                                                             percent_total,\n",
    "                                                                             percent_elite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User mentions on tweets\n",
    "\n",
    "In a similar way we can see user mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('user_mentions', [])\n",
    "    return [tag['screen_name'] for tag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    users = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        mentions_in_tweet = get_mentions(tweet)\n",
    "        users.update(mentions_in_tweet)\n",
    "    for user, count in users.most_common(20):\n",
    "        print(\"{}: {}\".format(user, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Text Analysis\n",
    "Now we are gonna read the text of the tweets and process some of the words, checking some statistics also. For this we will use the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case you have a problem like the one below, uncomment the following lines:\n",
    "![./Problem01.png](./Problem01.png)\n",
    "\n",
    "![./Problem01.png](./Solution01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK will be our Natural Language Library! You have to install it (possibly) to make the code work. So uncomment the code below if necessary, and after you install it, then comment it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    \"\"\"Process the text of a tweet:\n",
    "    - Lowercase\n",
    "    - Tokenize\n",
    "    - Stopword removal\n",
    "    - Digits removal\n",
    "\n",
    "    Return: list of strings\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # If we want to normalize contraction, uncomment this\n",
    "    tokens = normalize_contractions(tokens)\n",
    "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit() and tok != u'\\u2026'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_contractions(tokens):\n",
    "    \"\"\"Example of normalization for English contractions.\n",
    "\n",
    "    Return: generator\n",
    "    \"\"\"\n",
    "    token_map = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we'll\": \"we will\",\n",
    "    }\n",
    "    for tok in tokens:\n",
    "        if tok in token_map.keys():\n",
    "            for item in token_map[tok].split():\n",
    "                yield item\n",
    "        else:\n",
    "            yield tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('dutch') + punct + ['rt', 'via'] \n",
    "\n",
    "tf = Counter()\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet.get('text', ''),\n",
    "                         tokenizer=tweet_tokenizer,\n",
    "                         stopwords=stopword_list)\n",
    "        tf.update(tokens)\n",
    "    for tag, count in tf.most_common(30):\n",
    "        print(\"\\'{}\\': {}\".format(tag.encode('utf-8'), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting frequency\n",
    "\n",
    "Instead of printing the values of frequency we can create graphics that allow us to visualize the results better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('dutch') + punct + ['rt', 'via']\n",
    "\n",
    "\n",
    "tf = Counter()\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet.get('text', ''),\n",
    "                         tokenizer=tweet_tokenizer,\n",
    "                         stopwords=stopword_list)\n",
    "        tf.update(tokens)\n",
    "        \n",
    "    tags = [tag for tag, count in tf.most_common(30)]\n",
    "    \n",
    "    #print tags\n",
    "    y = [count for tag, count in tf.most_common(30)]\n",
    "    x = range(1, len(y)+1)\n",
    "\n",
    "    plt.figure(figsize=(14,7), dpi=300)\n",
    "\n",
    "    plt.bar(x, y, align='center', alpha=0.5)\n",
    "    plt.title(\"Term Frequencies\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(range(1,len(tags)+1), tags, rotation=90)\n",
    "    plt.savefig('term_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The same can be done for the hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('hashtags', [])\n",
    "    return [tag['text'].lower() for tag in hashtags]\n",
    "\n",
    "fname = 'user_timeline_VUamsterdam.jsonl'\n",
    "with open(fname, 'r') as f:\n",
    "    hashtags = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        hashtags_in_tweet = get_hashtags(tweet)\n",
    "        hashtags.update(hashtags_in_tweet)\n",
    "    for tag, count in hashtags.most_common(30):\n",
    "        print(\"{}: {}\".format(tag, count))\n",
    "\n",
    "    tags = [tag for tag, count in hashtags.most_common(30)]\n",
    "    \n",
    "    #print tags\n",
    "    y = [count for tag, count in hashtags.most_common(30)]\n",
    "    x = range(1, len(y)+1)\n",
    "\n",
    "    plt.figure(figsize=(14,7), dpi=300)\n",
    "\n",
    "    plt.bar(x, y, align='center', alpha=0.5)\n",
    "    plt.title(\"Hashtags Frequencies\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(range(1,len(tags)+1), tags, rotation=90)\n",
    "    plt.savefig('hashtags_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2\n",
    "You might have run the code up to now based on the original example of the VU Amsterdam profile. Pick a new profile on Twitter and run the first part again. Also change the code above to show the 1000 most common hashtags. \n",
    "\n",
    "Can you tell about the results you obtained?\n",
    "1. Why did you pick this page?\n",
    "2. What is interesting about it when you verify the hashtags and the most common words?\n",
    "3. What else would be interesting to do with the data obtained in this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "Streaming is a way to keep a 'door' open listening to the twitter flow of interactions over time. This allow us to collect more data than permited with all the restrictions of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_filename(fname):\n",
    "    \"\"\"Convert fname into a safe string for a file name.\n",
    "\n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if \"invalid\".\n",
    "\n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming Twitter data.\"\"\"\n",
    "\n",
    "    def __init__(self, fname):\n",
    "        safe_fname = format_filename(fname)\n",
    "        self.outfile = \"stream_%s.jsonl\" % safe_fname\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            sys.stderr.write(\"Error on_data: {}\\n\".format(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            sys.stderr.write(\"Rate limit exceeded\\n\".format(status))\n",
    "            return False\n",
    "        else:\n",
    "            sys.stderr.write(\"Error {}\\n\".format(status))\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **stream** will run for 120 seconds and the query is Ajax OR footbal. If you want the occurrence of two values together, like Ajax AND football, we should pass the query = ['Ajax footbal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = ['Ajax', 'football']\n",
    "query_fname = ' '.join(query) # string\n",
    "auth = get_twitter_auth()\n",
    "twitter_stream = Stream(auth, CustomListener(query_fname))\n",
    "twitter_stream.filter(track=query, async=True)\n",
    "time.sleep(120)\n",
    "# TO stop your streaming...\n",
    "twitter_stream.running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To stop your streaming before the time set...\n",
    "twitter_stream.running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3\n",
    "\n",
    "Now, create your own query, and pay attention on where are the results of the query being saved. Can you tell just looking at the code?\n",
    "\n",
    "Choose your query with wisdom, 'cause you are going to use it next. You can also increase the time if you want to capture more tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of the Tweets\n",
    "\n",
    "Change the file to your stream file in order to get the right dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "tweets = []\n",
    "text_tweets = []\n",
    "fileTweets = \"stream_Ajax_football.jsonl\"\n",
    "print fileTweets\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        #print tweet\n",
    "        try:\n",
    "            aux = { \"text\" : unidecode(tweet['text'].replace('\"','')), \"language\": tweet['lang'],  \"query\" : query, \"id\" : tweet['id'] }\n",
    "            text_tweets.append(tweet['text'])\n",
    "            tweets.append(aux)\n",
    "        except:\n",
    "            print 'no text', tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = { \"data\" : tweets }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_response(json_response):\n",
    "    negative_tweets, positive_tweets = 0, 0\n",
    "    for j in json_response[\"data\"]:\n",
    "        if int(j[\"polarity\"]) == 0:\n",
    "            negative_tweets += 1\n",
    "        elif int(j[\"polarity\"]) == 4:\n",
    "            positive_tweets += 1\n",
    "    return negative_tweets, positive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "URL_SENTIMENT140 = \"http://www.sentiment140.com/api/bulkClassifyJson\"\n",
    "req = urllib2.Request(URL_SENTIMENT140)\n",
    "req.add_header('Content-Type', 'application/json')\n",
    "response = urllib2.urlopen(req, str(result))\n",
    "json_response = json.loads(response.read())\n",
    "negative_tweets, positive_tweets = parse_response(json_response)\n",
    "\n",
    "print \"Positive Tweets: \" + str(positive_tweets)\n",
    "print \"Negative Tweets: \" + str(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4\n",
    "\n",
    "1. Print a list of 5 positive and 5 negative tweets using the positive_tweets and negative_tweets list. (the responses are in the variable json_response)\n",
    "2. Can you draw any hypotheses over why did you get this proportion of positive/negative tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Extra] Time Series Analysis\n",
    "**If you have time, and wanna learn more about time series analysis, leave your stream running longer (for one hour or two, for instance) and try the code below. From this part on won't be graded!**\n",
    "\n",
    "The time series analysis evaluates the tweets over time, giving us an idea about how many tweets were collected every minute. Remember to change the name of fname in case you changed your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'stream_Ajax_football.jsonl'\n",
    "\n",
    "with open(fname, 'r') as f:\n",
    "    all_dates = []\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        all_dates.append(tweet.get('created_at'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = np.ones(len(all_dates))\n",
    "idx = pd.DatetimeIndex(all_dates)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "my_series = pd.Series(ones, index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resampling / bucketing into 1-minute buckets\n",
    "per_minute = my_series.resample('1Min').sum().fillna(0)\n",
    "print(my_series.head())\n",
    "print(per_minute.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(True)\n",
    "ax.set_title(\"Tweet Frequencies\")\n",
    "\n",
    "hours = mdates.MinuteLocator(interval=1)\n",
    "date_formatter = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "datemin = datetime(2016, 11, 12, 18, 37)\n",
    "datemax = datetime(2016, 11, 12, 18, 47)\n",
    "\n",
    "ax.xaxis.set_major_locator(hours)\n",
    "ax.xaxis.set_major_formatter(date_formatter)\n",
    "ax.set_xlim(datemin, datemax)\n",
    "max_freq = per_minute.max()\n",
    "ax.set_ylim(0, max_freq+100)\n",
    "ax.plot(per_minute.index, per_minute)\n",
    "\n",
    "#ax.xticks(rotation=90)\n",
    "\n",
    "plt.savefig('tweet_time_series.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Extra] Users, Followers and Communities\n",
    "\n",
    "This section is not part of the assignment. Just read and play with it if you wanna learn a little bit of how to get users that are followers or friends of a certain node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = get_twitter_client()\n",
    "profile = client.get_user(screen_name='ericinlinux')\n",
    "print(json.dumps(profile._json, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "MAX_FRIENDS = 15000\n",
    "\n",
    "def paginate(items, n):\n",
    "    \"\"\"Generate n-sized chunks from items\"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "screen_name = 'ericinlinux'\n",
    "\n",
    "client = get_twitter_client()\n",
    "dirname = \"users/{}\".format(screen_name)\n",
    "max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "\n",
    "try:\n",
    "    \n",
    "    os.makedirs(dirname)\n",
    "except OSError:\n",
    "    print(\"Directory {} already exists\".format(dirname))\n",
    "except Exception as e:\n",
    "    print(\"Error while creating directory {}\".format(dirname))\n",
    "    print(e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get followers for a given user\n",
    "fname = \"users/{}/followers.jsonl\".format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    for followers in Cursor(client.followers_ids, screen_name=screen_name).pages(max_pages):\n",
    "        for chunk in paginate(followers, 100):\n",
    "            users = client.lookup_users(user_ids=chunk)\n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+\"\\n\")\n",
    "        if len(followers) == 5000:\n",
    "            print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "            time.sleep(60)\n",
    "\n",
    "# get friends for a given user\n",
    "fname = \"users/{}/friends.jsonl\".format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    for friends in Cursor(client.friends_ids, screen_name=screen_name).pages(max_pages):\n",
    "        for chunk in paginate(friends, 100):\n",
    "            users = client.lookup_users(user_ids=chunk)\n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+\"\\n\")\n",
    "        if len(friends) == 5000:\n",
    "            print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "            time.sleep(60)\n",
    "\n",
    "# get user's profile\n",
    "fname = \"users/{}/user_profile.json\".format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    profile = client.get_user(screen_name=screen_name)\n",
    "    f.write(json.dumps(profile._json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see a folder called users in the same directory as this file with the json files there. You can try to extract information about the JSON files and get more from here.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
